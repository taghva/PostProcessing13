\documentclass{article}
\usepackage{graphicx}
\usepackage{epsfig}

\marginparwidth 0pt
\marginparsep 0pt
\oddsidemargin 0.5in
\evensidemargin0.5in
\topmargin 0in
\textwidth 5.5in
\textheight 8in
\footskip 0.5in

\title{ Utilizing Web Data in Identification and Correction of OCR Errors}
\author{Kazem Taghva and Shivam Agarwal\\
Department of Computer Science\\
University of Nevada, Las Vegas\\
Las Vegas, NV }
\date{} % delete this line to display the current date

%%% BEGIN DOCUMENT
\begin{document}

\maketitle

\section{Abstract}

In this paper, we report on our experiments for detection and
correction of OCR errors with web data. More specifically, we utilize
Google search to access the big data resources available to identify
possible candidates for correction. We then use a combination of the
Longest Common Subsequences (LCS) and Bayesian estimates to
automatically pick the proper candidate.

Our experimental results on a small set of historical newspaper data
show a recall and precision of 51\% and 100\%, respectively. The work in
this paper further provides a detailed classification and analysis of
all errors. In particular, we point out the shortcomings of our
approach in its ability to suggest proper candidates to correct the
remaining errors.\\


{\bf Keywords:} Post Processing, Information Extraction, Mining, Error
Identification, Error Correction, Big Data

\section{Introduction}
\label{sec:introduction}

The conversion of a large collection of paper documents to electronic is
typically done using Optical Character Recognition followed by some
sort of post processing \cite{Taghva2006}. The latter is performed:
\begin{itemize}

\item To remove extraneous characters produced as a result of
  misrecognition of graphics.\\
\item To eliminate end-of-line hyphenations.\\
\item To possibly identify and correct word errors.

\end{itemize}

The post processing can be very expensive and labor intensive
depending on conversion requirements and textual applications. For
retrieval purposes, experiments have shown that average precision is
unaffected when comparing OCR text to its nearly perfect text version
\cite{Taghva96}.  It is also shown that some characteristics of OCR
text can cause problems for certain functionalities of Information
Retrieval (IR) and Information Extraction (IE)
systems\cite{Taghva2006}.

In this paper, we report on an experiment to automatically identify
and correct OCR errors based on the availability of big data on the
web. 

In section~\ref{sec:background}, we provide some background and
related work on error identification and
correction. Section~\ref{sec:web} describes our motivation for use of
web data for error correction.  Section~\ref{sec:algorithm} is the
description of our algorithm. Section~\ref{sec:experiment} is a
summary of our results, and section~\ref{sec:conclusion} is a
conclusion and proposes future work.

\section{Background}
\label{sec:background}

Identification and correction of misspellings in text has a long
history and many of techniques are summarized by Kukich \cite{Kukich}.
Other notable work in this area are the work of Brill \cite{Brill} and
Hauser et al.\cite{Hauser}. The latter deals with identification of
various spelling forms of words in historical documents. Most of the
methodologies cited in these papers deal with typical spelling errors
caused by the arrangement of alphabets on the computer key boards. The
OCR errors are due to character segmentation and recognition and hence
are less responsive to standard spelling correction techniques.

The character accuracy of an OCR engine is obviously a major factor in
the number of misspellings in a converted document. A standard text
page has about 2500 characters and an OCR engine with 99\% character
accuracy can produce approximately 25 errors. This does not translate
to 25 misspellings as character errors tend to be 1.5 errors per
misrecognized word on the average. As a result, 99\% character
accuracy produces around 15 misspelling per page.

Many studies have been done to understand the nature of OCR errors and
possibly suggest remedies for specific text applications such as
retrieval or extraction. One such approach is proposed by Jin and
Hauptman \cite{Jin2003} that incorporates a general content-based
correction model that works on the top of an existing OCR correction
tool to boost retrieval performance. An example of an OCR correction tool
is OCRspell proposed by Taghva and Stofsky \cite{Taghva01}.  OCRspell
is especially designed as a semi-automatic approach.  A learning
mechanism is used based on the corrections applied by the user. By
comparing error-prone tokens and the manual replacement by the user,
dynamic mappings are derived. For instance, from iiiount@in $
\rightarrow $ mountain the mappings iii $ \rightarrow $ m and @ $
\rightarrow $ a are derived.

The Hauser\cite{Hauser} work is interesting in the sense that
supervised and unsupervised learning are applied to solve spelling
variation of words in historical documents. Taghva \cite{Taghva2013}
used supervised learning capabilities of Hidden Markov Model (HMMs) to
identify and correct OCR errors in a post processing application. This
work showed that the second order HMM does not improve the accuracy
over the first order when we require high precision despite the
additional context.

In the next section, we report on our work that uses the word sequence
and context to correct OCR errors. We will point out that the context
beyond four surrounding words does not contribute to more corrections.

\section{Web Data and OCR Post Processing}
\label{sec:web}
The training of HMM based post processing systems collects statistics
on character frequency and sequencing to calculate probabilities for
decoding of errors via Viterbi. Based on \cite{Taghva2013}, the HMM
can correct about 11\% of the errors at precision of 100\%. In other
words, we get 11\% improvement without corrupting the text by
replacing a correct word with an incorrect one. We are of the opinion
that 100\% precision is a good requirement for an ideal post
processor. Most of us have observed that spell checkers cannot detect
a class of errors in which one correct word is substituted for another
correct word erroneously. These type of errors require more context
information for identification and correction. One such approach based
on statistical language modeling is proposed by Tong and
Evans\cite{Tong96}. Another example of a post processor that uses
phrases and document content to correct errors is Manicure
\cite{Taghva98}.

One recent work \cite{Bassil2012, Bassil2012b} uses Google’s Web 1T
5-gram data set to detect and correct OCR spelling. The algorithm
works on an OCR text file as follows:

\begin{enumerate}

\item Each word is spell checked against Google Web 1T unigram to
  identify all mispellings in the file.\\

\item Each misspelling is matched with Google Web 1T to identify the
  top 10 correctly spelled words based on the number of common bigrams.\\

\item Each misspelling along with four preceding words in the text
  file is matched with Google Web 1T to identify five-word
  sequences.\\

\end{enumerate}

The correct spelling is picked from the ten candidates based on the
ranking of the five-word sequences. The algorithm shows good potential
based on a small experiment by authors.

We see at least two ways that the algorithm can be improved. The first
one is the fact that OCR errors are different than other spelling
errors. For example, we do not type {\tt rn} for {\tt m}. The second
one is the fact that document content beyond five-word sequencing can
play a role in error correction. We incorporate both of these
observations in our algorithm and show examples that can benefit from
these changes.

\section{Error Correction Algorithm}
\label{sec:algorithm}

The proposed OCR error correction starts by first cleaning OCR corpus
$T$ to remove all characters other than {\tt a} to {\tt z} as well as
all the stopwords.  Then the cleaned text $T_c$ is screened through
spell checker {\tt Jspell} which produces the set of all probable
errors $E$.

We concatenate each OCR error with words immediately
preceding or following it to generate queries of variable length.
Formally each query $Q$ can be denoted as: $Q = w_{-n}, \dots w_{-2}
,w_{-1} ,e, w_1,w_2, \dots w_n$, where $w_{-i}$ represents the ith
word that precedes $e$, and $w_i$ represent ith word following
$e$. The number of words $2n+1$ can be theoretically as large as one
wishes but in our experiments, it ranges over 1,3,5,7 and 9.  The
query $Q$ is then fed to Google as a search string. 

The algorithm selects the {\tt HTML} summary of the top ten ranked
documents for further processing. These summaries are parsed to pick
the highlighted words to be used as correction candidates for the
error $e$. The Google`s result page is also searched for Google`s
{\em Did you mean} or {\em Showing results for} .  If any suggestion is
found then it is appended to the list of Correction Candidates. Let
this list be $C = \{ c_1, c_2, \dots c_n \}$. Both Levenstein Edit
distance and LCS are used to identify candidates $c_j$ and $c_k$ for
correction of error $e$, respectively. Since the possibility that $e$
may be correctly spelled exists, we allow edit distance 0. In case
there are more than one best candidate $c_j$ or $c_k$, we use the OCR
confusion matrix $M$ to pick a candidate. The matrix $M$ is used to
compute the Bayes probability $p(c \mid e)$ ,the probability of $c$
being the correct candidate given error $e$.

In the next section, we provide details on our data, confusion matrix,
and experimental results.



\section{Experimental Results}
\label{sec:experiment}

To evaluate the performance of our approach, we need to determine the
evaluation measures. There are four possible outcomes that can occur
when applying a post processing to correct OCR errors:

\begin{enumerate}
\item correct $\rightarrow$ correct: A correct character is still
  correct after processing. This is a true negative (TN). \\

\item correct $\rightarrow$ wrong: A correct character is changed to a
  wrong character at by the process. This is a false positive (FP). \\

\item wrong $\rightarrow$ correct: A character is corrected by the
  procedure. This is a true positive (TP). \\

\item wrong $\rightarrow$ wrong: A wrong character is still wrong
  after the process. This is a false negative (FN). \\

\end{enumerate}

Recall and Precision values are calculated using TP, FP and FN as
follows: \\ Recall = $\frac{TP }{TP+FN }$\\ Precision = $\frac{TP
}{TP+FP}$\\ The definition of these measures has been proposed and
discussed by Reynaert \cite{Reynaert}. Recall values show the ability
of the system to correct errors and the precision values indicate the
accuracy of the system.

In order to test the performance of our approach, we used two small
data sets. The first set was used to build a generic confusion
matrix. This data has been taken from a book titled “Notes on
Witchcraft” with 60 pages, which has been manually corrected with
reference to a non-OCR version image of the book. The second set of
data is taken from Library of
Congress \begin{quote}(http://www.loc.gov/index.html)\end{quote}.

We picked seven pages based on various criteria such as readability,
date of publication, and convenience to map with its corresponding OCR
text.  These pages are from newspaper articles written between 1836 to
1922 and have been digitized as grayscale with 400 dpi
resolution. There are ninety five misspellings in these pages. We are
interested in having a high recall at precision of close to 100\%. The
table~\ref{tab:performance} shows the recall and precision values
associated with queries of different length.

\begin{table}
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}\label{tab:performance}
Distance & 1 word&3 words &5 words &7 words& 9 words\\
\hline
Levenstein& R=32&R=44&R=48.5&R=43&R=40\\
          & P=100&P=100&P=100&P=100&P=100\\
\hline
LCS&R=30&R=45&R=51&R=44&R=41\\
   & P=100&P=100&P=100&P=100&P=100\\
\hline
\end{tabular}
\caption{The Performance at Different Query Length}
\end{center}
\end{table}

Table~\ref{tab:performance} shows that recall is the lowest value for
the one word query.  The value increases as query length expands.
It reaches its maximum value 51.5\% at a query length
five. The LCS method gives the highest values for recall at 100\%
Precision. 

It is observed that our approach corrects more errors than the
correction suggested by Google`s {\em Did you mean}. The Google`s
suggestion only, corrects 42 errors out of 95 misspellings but our
approach is able to correct 49 misspellings. This is an improvement of
around 16.6\%. More analysis of data shows that if the context
surrounding the error is also misspelled then Google fails to suggest
a candidate. For instance, the query “tne+territory+mnke+advances+tho”
does not get a candidate for {\tt mnke} by Google. Further Google has
no suggestion for any word in the query
“day+Oclock+Mrther+tf+jnd”. Our approach corrects both {\tt mnke} and
{\tt Mrther}. The confusion matrix also provides correction for some
words. For example, in the query “work underway roaa surVs Yucca”,
the error {\tt roaa} gets corrected by higher probability of {\tt rosa}
over {\tt road}.

\section{Conclusion and Future Work}
\label{sec:conclusion}

This paper is a preliminary explanation of our project. We have shown
that the use of document content can improve the post processing
beyond Google`s {\em Did you mean} suggestions. We have also shown
that the use of a confusion matrix can improve the automated
correction procedure for OCR text. As it is stated, this is a
preliminary experiment and we intend to report on extended work in the
future.

\bibliographystyle{plain}
\bibliography{/home/taghva/Documents/Research/references}


\end{document}
